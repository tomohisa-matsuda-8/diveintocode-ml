{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sprint_15.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"O10z2npdbstJ"},"source":["## Sprint 15 問題"]},{"cell_type":"markdown","metadata":{"id":"8frMypUHdEBq"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"f6tYawOsb65v"},"source":["### (1) 物体検出の分野にはどういった手法が存在したか。"]},{"cell_type":"markdown","metadata":{"id":"dCG3eyA2cctb"},"source":["【回答】\n","\n","SPPnetやFast R-CNNのような手法が存在した。領域提案の計算時間を減らすことが課題だった。"]},{"cell_type":"markdown","metadata":{"id":"WDpmClMRcBoc"},"source":["【引用箇所】P.1 Abstract\n","\n","State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. "]},{"cell_type":"markdown","metadata":{"id":"5lmjf1mogYRf"},"source":["【引用箇所】 Fast R-CNN (arXiv:1504.08083) P.1\n","\n","1. Introduction\n"," 1.1. R-CNN and SPPnet\n","  The Region-based Convolutional Network method (R- CNN) [9] achieves excellent object detection accuracy by using a deep ConvNet to classify object proposals. R-CNN, however, has notable drawbacks〜"]},{"cell_type":"markdown","metadata":{"id":"GKraortqcBlM"},"source":["### (2) Fasterとあるが、どういった仕組みで高速化したのか。"]},{"cell_type":"markdown","metadata":{"id":"R3-JDEQ6cBiX"},"source":["【回答】\n","\n","物体検出ネットワークと畳み込み層を共有する新しい領域提案ネットワーク（RPN）を導入し、テスト時に畳み込みを共有することで，提案を計算するための計算量を減らし高速化できる"]},{"cell_type":"markdown","metadata":{"id":"ERbK9UlkssmK"},"source":["【引用箇所】 P.1 INTRODUCTION\n","\n","To this end, we introduce novel Region Proposal Networks (RPNs) that share convolutional layers with state-of-the-art object detection networks [1], [2]. By sharing convolutions at test-time, the marginal cost for computing proposals is small (e.g., 10ms per image)."]},{"cell_type":"markdown","metadata":{"id":"QUjmlG4fcBd1"},"source":["### (3) One-Stageの手法とTwo-Stageの手法はどう違うのか。"]},{"cell_type":"markdown","metadata":{"id":"5uKbzCDtcBWQ"},"source":["【回答】\n","\n","Two-Stage: 畳み込み特徴マップ上のスライディングウィンドウに回帰器と分類器を用いた検出方法\n","\n","One-Stage: OverFeatのような1段階のクラス別検出パイプライン"]},{"cell_type":"markdown","metadata":{"id":"b9JtFCzUt2jt"},"source":["【引用箇所】P.10 One-Stage Detection vs. Two-Stage Proposal + De- tection.\n","\n","\n","The OverFeat paper [9] proposes a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps. OverFeat is a one-stage, class-specific detection pipeline, and ours is a two-stage cascade consisting of class-agnostic proposals and class-specific detections."]},{"cell_type":"markdown","metadata":{"id":"cgNzj3y8cBNn"},"source":["### (4) RPNとは何か。"]},{"cell_type":"markdown","metadata":{"id":"kglpdbk6cBAJ"},"source":["【回答】\n","\n","RPN (Region Proposal Network)は、画像全体からCNNで抽出した特徴マップに対して、候補領域のBounding Boxと、その領域の物体らしさ(Objectness)を表すスコアを出力する。フル画像の畳み込み特徴量を検出ネットワークと共有することで，ほぼコストをかけずに領域提案を行うことができる"]},{"cell_type":"markdown","metadata":{"id":"X1g4E1nXvDLx"},"source":["【引用箇所】P.3 3.1 Region Proposal Networks\n","\n","A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score.3 We model this process with a fully convolutional network [7], which we describe in this section. Because our ultimate goal is to share computation with a Fast R-CNN object detection network [2], we assume that both nets share a common set of convolutional layers."]},{"cell_type":"markdown","metadata":{"id":"yErh9pWmcJUy"},"source":["### (5) RoIプーリングとは何か。"]},{"cell_type":"markdown","metadata":{"id":"ZS_t1R51cJP9"},"source":["【回答】\n","RoIプーリング層は、maxプーリングを用いて\n","興味のある有効な領域内の特徴を、H×Wの固定された空間エクステント（例：7×7）の小さな特徴マップに変換する。（アスペクト比の違いを考慮してMax Poolingを行う処理。検出領域を固定サイズのベクトルに収める。このベクトルが、①候補領域(物体)の分類(Classification)タスクを学習する全結合層、②\n","Bounding Boxの回帰(Regression)タスクを学習する全結合層への入力となる）\n"]},{"cell_type":"markdown","metadata":{"id":"nJ1z5yEnvn3j"},"source":["【引用箇所】 Fast R-CNN (arXiv:1504.08083) P.2\n","\n","2.1. The RoI pooling layer\n","The RoI pooling layer uses max pooling to convert the\n","features inside any valid region of interest into a small feature map with a fixed spatial extent of H × W (e.g., 7 × 7),\n"]},{"cell_type":"markdown","metadata":{"id":"NNj27L1-cJJX"},"source":["### (6) Anchorのサイズはどうするのが適切か。"]},{"cell_type":"markdown","metadata":{"id":"S4APbujJcJF2"},"source":["【回答】\n","\n","3つのスケール(1つのアスペクト比)または3つのアスペクト比(1つのスケール)を使用するとmAPは高くなり、複数のサイズのアンカーを回帰参照として使用することが効果的である。"]},{"cell_type":"markdown","metadata":{"id":"NInxDI_gx5v3"},"source":["【引用箇所】P.9 Sensitivities to Hyper-parameters.\n","\n","The mAP is higher if using 3 scales (with 1 aspect ratio) or 3 aspect ratios (with 1 scale), demonstrating that using anchors of multiple sizes as the regression references is an effective solution. Using just 3 scales with 1 aspect ratio (69.8%) is as good as using 3 scales with 3 aspect ratios on this dataset, suggesting that scales and aspect ratios are not disentangled dimensions for the detection accuracy"]},{"cell_type":"markdown","metadata":{"id":"q6ZRDwr-cOQO"},"source":["###(7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。"]},{"cell_type":"markdown","metadata":{"id":"RwoB2HU4cOrD"},"source":["【回答】\n","PASCAL VOC 2007検出ベンチマークにて、検出の平均平均精度（mAP）を評価した。下記の通りという競争力のある結果が得られた。\n","\n","・Selective Search：58.7%\n","\n","・EdgeBoxes       ：58.6%\n","\n","・RPN +Fast R-CNN ：59.9% "]},{"cell_type":"markdown","metadata":{"id":"-PG27dWhzFxO"},"source":["【引用箇所】P.7 4.1 Experiments on PASCAL VOC\n","\n","We comprehensively evaluate our method on the PASCAL VOC 2007 detection benchmark [11]. This dataset consists of about 5k trainval images and 5k test images over 20 object categories. We also provide results on the PASCAL VOC 2012 benchmark for a few models. For the ImageNet pre-trained network, we use the “fast” version of ZF net [32] that has 5 convolutional layers and 3 fully-connected layers, and the public VGG-16 model7 [3] that has 13 convolutional layers and 3 fully-connected layers. We primarily evaluate detection mean Average Precision (mAP), because this is the actual metric for object detection (rather than focusing on object proposal proxy metrics).\n","Table 2 (top) shows Fast R-CNN results when trained and tested using various region proposal methods. These results use the ZF net. For Selective Search (SS) [4], we generate about 2000 proposals by the “fast” mode. For EdgeBoxes (EB) [6], we generate the proposals by the default EB setting tuned for 0.7 IoU. SS has an mAP of 58.7% and EB has an mAP of 58.6% under the Fast R-CNN framework. RPN with Fast R-CNN achieves competitive results, with an mAP of 59.9% while using up to 300 proposals8. Using RPN yields a much faster detection system than using either SS or EB because of shared convolutional computations; the fewer proposals also reduce the region-wise fully-connected layers’ cost (Table 5)."]},{"cell_type":"code","metadata":{"id":"uCdY8S2HzCqh"},"source":[""],"execution_count":null,"outputs":[]}]}