{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】クロスバリデーション\n",
    "事前学習期間では検証データをはじめに分割しておき、それに対して指標値を計算することで検証を行っていました。（ホールドアウト法）しかし、分割の仕方により精度は変化します。実践的には クロスバリデーション（交差検証） を行います。分割を複数回行い、それぞれに対して学習と検証を行う方法です。複数回の分割のためにscikit-learnにはKFoldクラスが用意されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.0, ACC : 0.9160528754694893\n",
      "No.1, ACC : 0.9161829504251825\n",
      "No.2, ACC : 0.9166056940311855\n",
      "No.3, ACC : 0.9157276880802563\n",
      "No.4, ACC : 0.9159390598832577\n",
      "ACCの平均: 0.9161016535778742\n",
      "No.0, AUC : 0.5507189623767521\n",
      "No.1, AUC : 0.550369544217318\n",
      "No.2, AUC : 0.5454073552687767\n",
      "No.3, AUC : 0.5521501503113658\n",
      "No.4, AUC : 0.5476827519428941\n",
      "AUCの平均: 0.5492657528234213\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (roc_curve, auc, accuracy_score)\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#csvファイルの読込\n",
    "df_train = pd.read_csv(\"application_train.csv\")\n",
    "df_test = pd.read_csv(\"application_test.csv\")\n",
    "\n",
    "#使用する特徴量\n",
    "features = [\"AMT_INCOME_TOTAL\", \"REGION_RATING_CLIENT\", \"DAYS_BIRTH\", \"DAYS_ID_PUBLISH\"]\n",
    "\n",
    "#trainデータの特徴量と目的変数の抽出\n",
    "X = df_train.loc[:,features].values\n",
    "y = df_train.loc[:,\"TARGET\"].values\n",
    "\n",
    "#train, testデータへの分割\n",
    "X_init_train, X_init_test, y_init_train, y_init_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "#標準化\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_init_train)\n",
    "X_train_transformed = scaler.transform(X_init_train)\n",
    "X_test_transformed = scaler.transform(X_init_test)\n",
    "\n",
    "#クロスバリデーション\n",
    "accuracy_list = []\n",
    "auc_list = []\n",
    "kf = KFold(n_splits=5)\n",
    "for train_index, test_index in kf.split(X_train_transformed):\n",
    "    X_train_cross, X_test_cross = X_train_transformed[train_index], X_train_transformed[test_index]\n",
    "    y_train_cross, y_test_cross = y_init_train[train_index], y_init_train[test_index]\n",
    "\n",
    "    #ランダムフォレストにて学習\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train_cross, y_train_cross)\n",
    "    \n",
    "    #精度予測\n",
    "    y_pred = rf.predict(X_test_transformed)\n",
    "    score = accuracy_score(y_init_test, y_pred)\n",
    "    accuracy_list.append(score)\n",
    "\n",
    "    #ROC予測\n",
    "    y_pred = rf.predict_proba(X_test_transformed)\n",
    "    fpr, tpr, thresholds = roc_curve(y_init_test, y_pred[:,1], pos_label=1)\n",
    "    auc_list.append(auc(fpr, tpr))\n",
    "    \n",
    "#予測精度, AUCの表示\n",
    "for i, acc in enumerate(accuracy_list):\n",
    "    print(\"No.{}, ACC : {}\".format(i, acc))\n",
    "print(\"ACCの平均: {}\".format(np.mean(accuracy_list)))\n",
    "for i, auc in enumerate(auc_list):\n",
    "    print(\"No.{}, AUC : {}\".format(i, auc))\n",
    "print(\"AUCの平均: {}\".format(np.mean(auc_list)))\n",
    "\n",
    "#test.csvの特徴量の抽出\n",
    "X_test_features = df_test.loc[:,features].values\n",
    "\n",
    "#test.csv特徴量の標準化\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_test_features)\n",
    "X_test_features_transformed = scaler.transform(X_test_features)\n",
    "\n",
    "#test.csv特徴量からデフォルト予測\n",
    "y_test_pred = rf.predict_proba(X_test_features_transformed)\n",
    "\n",
    "#以下、Kaggle提出用データの作成\n",
    "df_X_test_ID = df_test.loc[:,[\"SK_ID_CURR\"]]\n",
    "df_y_pred = pd.DataFrame(y_test_pred[:,1])\n",
    "df_output = pd.concat([df_X_test_ID, df_y_pred], axis=1)\n",
    "df_output.columns = [\"SK_ID_CURR\", \"TARGET\"]\n",
    "df_output.to_csv('home_creditdata1.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】グリッドサーチ\n",
    "これまで分類器のパラメータには触れず、デフォルトの設定を使用していました。パラメータの詳細は今後のSprintで学んでいくことになります。機械学習の前提として、パラメータは状況に応じて最適なものを選ぶ必要があります。最適なパラメータを探していくことを パラメータチューニング と呼びます。パラメータチューニングをある程度自動化する単純な方法としては グリッドサーチ があります。\n",
    "\n",
    "scikit-learnのGridSearchCVを使い、グリッドサーチを行うコードを作成してください。そして、ベースラインモデルに対して何らかしらのパラメータチューニングを行なってください。どのパラメータをチューニングするかは、使用した手法の公式ドキュメントを参考にしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".best_estimator_の内容: RandomForestClassifier(max_depth=5, n_estimators=120)\n",
      "0.9194579038495918 {'max_depth': 5, 'n_estimators': 120}\n",
      "0.9194579038495918 {'max_depth': 5, 'n_estimators': 300}\n",
      "0.915628758577887 {'max_depth': None, 'n_estimators': 120}\n",
      "0.9158563937489961 {'max_depth': None, 'n_estimators': 300}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n.best_estimator_の内容: RandomForestClassifier(max_depth=5, n_estimators=120)\\n0.9190229586279912 {'max_depth': 5, 'n_estimators': 120}\\n0.9190229586279912 {'max_depth': 5, 'n_estimators': 300}\\n0.9156612791509164 {'max_depth': None, 'n_estimators': 120}\\n0.9157059945154163 {'max_depth': None, 'n_estimators': 300}\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#csvファイルの読込\n",
    "df_train = pd.read_csv(\"application_train.csv\")\n",
    "df_test = pd.read_csv(\"application_test.csv\")\n",
    "\n",
    "#使用する特徴量\n",
    "features = [\"AMT_INCOME_TOTAL\", \"REGION_RATING_CLIENT\", \"DAYS_BIRTH\", \"DAYS_ID_PUBLISH\"]\n",
    "\n",
    "#trainデータの特徴量と目的変数の抽出\n",
    "X = df_train.loc[:,features].values\n",
    "y = df_train.loc[:,\"TARGET\"].values\n",
    "\n",
    "#train, testデータへの分割\n",
    "X_init_train, X_init_test, y_init_train, y_init_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "#標準化\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_init_train)\n",
    "X_train_transformed = scaler.transform(X_init_train)\n",
    "X_test_transformed = scaler.transform(X_init_test)\n",
    "\n",
    "#グリッドサーチ実行\n",
    "param = {'max_depth':[5,None],\n",
    "             'n_estimators':[120, 300]}\n",
    "\n",
    "rf = GridSearchCV(RandomForestClassifier(),   param)\n",
    "rf.fit(X_train_transformed, y_init_train) \n",
    "\n",
    "best_rf = rf.best_estimator_\n",
    "print('.best_estimator_の内容:', best_rf)\n",
    "\n",
    "scores = rf.cv_results_['mean_test_score']\n",
    "params = rf.cv_results_['params']\n",
    "for i in range(len(scores)):\n",
    "    print(scores[i], params[i])\n",
    "    \n",
    "\"\"\"\n",
    ".best_estimator_の内容: RandomForestClassifier(max_depth=5, n_estimators=120)\n",
    "0.9190229586279912 {'max_depth': 5, 'n_estimators': 120}\n",
    "0.9190229586279912 {'max_depth': 5, 'n_estimators': 300}\n",
    "0.9156612791509164 {'max_depth': None, 'n_estimators': 120}\n",
    "0.9157059945154163 {'max_depth': None, 'n_estimators': 300}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】Kaggle Notebooksからの調査\n",
    "KaggleのNotebooksから様々なアイデアを見つけ出して、列挙してください。\n",
    "\n",
    "1.オブジェクト変数の数値化、特徴量を増やす、欠損値をなくす\n",
    "\n",
    "2.異常値(勤続1000年等）に何かしらの値を代入\n",
    "\n",
    "3.モデルにLightGBMを使う\n",
    "\n",
    "4.多項式で新しい特徴量を作る。（PolynomialFeatures)\n",
    "\n",
    "【参考】\n",
    "\n",
    "https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction\n",
    "\n",
    "https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】高い汎化性能のモデル作成\n",
    "問題3で見つけたアイデアと、独自のアイデアを組み合わせ高い汎化性能のモデル作りを進めてください。\n",
    "\n",
    "その過程として、何を行うことで、クロスバリデーションの結果がどの程度変化したかを表にまとめてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】最終的なモデルの選定\n",
    "最終的にこれは良いというモデルを選び、推定した結果をKaggleに提出してスコアを確認してください。\n",
    "\n",
    "どういったアイデアを取り入れ、どの程度のスコアになったかを記載してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | 課題１ |課題４|\n",
    "| :---: | :---: | :---: | \n",
    "| モデル | ランダムフォレスト | ランダムフォレスト |\n",
    "| 特徴量の数 | 4 | 15 |\n",
    "| AUC算出値 | 0.549 | 0.717 |\n",
    "| Kaggleスコア |0.544 |  0.697 |\n",
    "|その他||オブジェクト変数を数値化<br>よりよい特徴量を使用できた|\n",
    "|||異常値の処理を実施<br>適切な機械学習ができたと考える|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.0, ACC : 0.9183617059330439\n",
      "No.1, ACC : 0.9186218558444303\n",
      "No.2, ACC : 0.9183942246719672\n",
      "No.3, ACC : 0.9186218558444303\n",
      "No.4, ACC : 0.9183779653025056\n",
      "ACCの平均: 0.9184755215192755\n",
      "No.0, AUC : 0.7169697466000684\n",
      "No.1, AUC : 0.7163242996073547\n",
      "No.2, AUC : 0.7180129591898027\n",
      "No.3, AUC : 0.7160663969554535\n",
      "No.4, AUC : 0.7179230884105645\n",
      "AUCの平均: 0.7170592981526488\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (roc_curve, auc, accuracy_score)\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#csvファイルの読込\n",
    "df_train = pd.read_csv(\"application_train.csv\")\n",
    "\n",
    "#データ前処理\n",
    "#オブジェクト変数をワンホットエンコーディングする。\n",
    "# 例：CODE_GENDER →　CODE_GENDER_M, CODE_GENDER_F, CODE_GENDER_XNAの３列に分割して0,1化\n",
    "df_train = pd.get_dummies(df_train)\n",
    "#df_train.to_csv('dummies.csv',index = False)\n",
    "\n",
    "#欠損値を0で補完する。\n",
    "df_train.fillna(0, inplace=True)\n",
    "#df_train.to_csv('fillna.csv',index = False)\n",
    "\n",
    "#異常値を削除する。　勤続年数1000年をその他の数値の平均に置換\n",
    "max_value = df_train[\"DAYS_EMPLOYED\"].max()\n",
    "median_value = df_train[\"DAYS_EMPLOYED\"].median()\n",
    "df_train = df_train.replace({\"DAYS_EMPLOYED\": {max_value: median_value}})\n",
    "#print(mean_value)\n",
    "\n",
    "#使用する特徴量 ポアソン相関係数が比較的高かったもの＋任意の特徴量の合計10個\n",
    "features = [\"AMT_INCOME_TOTAL\", \n",
    "#\"CNT_CHILDREN\", \n",
    "\"AMT_CREDIT\", \n",
    "\"DAYS_BIRTH\", \n",
    "\"DAYS_EMPLOYED\", \n",
    "#\"DAYS_REGISTRATION\", \n",
    "\"DAYS_ID_PUBLISH\", \n",
    "\"FLAG_EMP_PHONE\", \n",
    "\"REGION_RATING_CLIENT_W_CITY\", \n",
    "\"EXT_SOURCE_1\", \n",
    "\"EXT_SOURCE_2\", \n",
    "\"EXT_SOURCE_3\", \n",
    "\"NAME_CONTRACT_TYPE_Cash loans\", \n",
    "#\"NAME_CONTRACT_TYPE_Revolving loans\", \n",
    "\"FLAG_OWN_CAR_N\",  \n",
    "#\"NAME_TYPE_SUITE_Children\", \n",
    "\"NAME_TYPE_SUITE_Family\", \n",
    "\"NAME_FAMILY_STATUS_Married\", \n",
    "\"CODE_GENDER_F\", \n",
    "           ]\n",
    "\n",
    "#trainデータの特徴量と目的変数の抽出\n",
    "X = df_train.loc[:,features].values\n",
    "y = df_train.loc[:,\"TARGET\"].values\n",
    "\n",
    "#train, testデータへの分割\n",
    "X_init_train, X_init_test, y_init_train, y_init_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "#標準化\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_init_train)\n",
    "X_train_transformed = scaler.transform(X_init_train)\n",
    "X_test_transformed = scaler.transform(X_init_test)\n",
    "\n",
    "#クロスバリデーション\n",
    "accuracy_list = []\n",
    "auc_list = []\n",
    "kf = KFold(n_splits=5)\n",
    "for train_index, test_index in kf.split(X_train_transformed):\n",
    "    X_train_cross, X_test_cross = X_train_transformed[train_index], X_train_transformed[test_index]\n",
    "    y_train_cross, y_test_cross = y_init_train[train_index], y_init_train[test_index]\n",
    "\n",
    "    #ランダムフォレストにて学習\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train_cross, y_train_cross)\n",
    "    \n",
    "    #精度予測\n",
    "    y_pred = rf.predict(X_test_transformed)\n",
    "    score = accuracy_score(y_init_test, y_pred)\n",
    "    accuracy_list.append(score)\n",
    "\n",
    "    #ROC予測\n",
    "    y_pred = rf.predict_proba(X_test_transformed)\n",
    "    fpr, tpr, thresholds = roc_curve(y_init_test, y_pred[:,1], pos_label=1)\n",
    "    auc_list.append(auc(fpr, tpr))\n",
    "    \n",
    "#予測精度, AUCの表示\n",
    "for i, acc in enumerate(accuracy_list):\n",
    "    print(\"No.{}, ACC : {}\".format(i, acc))\n",
    "print(\"ACCの平均: {}\".format(np.mean(accuracy_list)))\n",
    "for i, auc in enumerate(auc_list):\n",
    "    print(\"No.{}, AUC : {}\".format(i, auc))\n",
    "print(\"AUCの平均: {}\".format(np.mean(auc_list)))\n",
    "\n",
    "\n",
    "#test.csvからテスト用データの作成（トレインデータと同様）\n",
    "df_test = pd.read_csv(\"application_test.csv\")\n",
    "df_test = pd.get_dummies(df_test)\n",
    "df_test.fillna(0, inplace=True)\n",
    "max_value = df_test[\"DAYS_EMPLOYED\"].max()\n",
    "median_value = df_test[\"DAYS_EMPLOYED\"].median()\n",
    "df_test = df_test.replace({\"DAYS_EMPLOYED\": {max_value: median_value}})\n",
    "\n",
    "#test.csvの特徴量の抽出\n",
    "X_test_features = df_test.loc[:,features].values\n",
    "\n",
    "#test.csv特徴量の標準化\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_test_features)\n",
    "X_test_features_transformed = scaler.transform(X_test_features)\n",
    "\n",
    "#test.csv特徴量からデフォルト予測\n",
    "y_test_pred = rf.predict_proba(X_test_features_transformed)\n",
    "\n",
    "#以下、Kaggle提出用データの作成\n",
    "df_X_test_ID = df_test.loc[:,[\"SK_ID_CURR\"]]\n",
    "df_y_pred = pd.DataFrame(y_test_pred[:,1])\n",
    "df_output = pd.concat([df_X_test_ID, df_y_pred], axis=1)\n",
    "df_output.columns = [\"SK_ID_CURR\", \"TARGET\"]\n",
    "df_output.to_csv('home_creditdata_4.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
